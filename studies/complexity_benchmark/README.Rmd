---
output:
  github_document:
    toc: false
    fig_width: 10.08
    fig_height: 6
tags: [r, complexity]
vignette: >
  %\VignetteIndexEntry{README}
  \usepackage[utf8]{inputenc}
  %\VignetteEngine{knitr::rmarkdown}
editor_options:
  chunk_output_type: console
bibliography: bibliography.bib
csl: utils/apa.csl
title: "**The Structure of Chaos: An Empirical Comparison of Fractal Physiology Complexity Indices using NeuroKit2**"
---

```{r, echo = FALSE, warning=FALSE, message=FALSE}
# options and parameters
options(digits = 3)

knitr::opts_chunk$set(
  collapse = TRUE,
  dpi = 450,
  fig.path = "../../studies/complexity_benchmark/figures/"
)
```



<!-- # Benchmarking and Analysis of Complexity Measures -->
<!-- # Measuring Chaos: Complexity and Fractal Physiology using NeuroKit2 -->
<!-- # Measuring Chaos with NeuroKit2: An Empirical Comparison of Fractal Physiology Complexity Indices -->
<!-- # The Structure of Chaos: An Empirical Comparison of Fractal Physiology Complexity Indices using NeuroKit2 -->

*This study can be referenced by* [*citing the package and the documentation*](https://neuropsychology.github.io/NeuroKit/cite_us.html).

**We'd like to improve this study, but unfortunately we currently don't have the time. If you want to help to make it happen, please contact us!**



```{r, child=c('analysis.Rmd')}
```



## Results

The data analysis script, the data and the code for the figures is fully available at **ADD LINK**. The analysis was performed in R using the *easystats* collection of packages [@correlationArticle, @seeArticle, @parametersArticle, @modelbasedPackage].

```{r message=FALSE, warning=FALSE, results='hide', echo=FALSE}
df <- read.csv("data_Complexity.csv") |>
  mutate(Method = as.factor(Method))
```

```{r message=FALSE, warning=FALSE, include=FALSE}
colors <- c(
  "SD" = "red",
  "Noise" = "red",
  "Length" = "red",
  "Random" = "red",
  "Frequency" = "red",
  "PFD (A)" = "#2196F3",
  "PFD (B)" = "#2196F3",
  "PFD (C)" = "#2196F3",
  "PFD (D)" = "#2196F3",
  "PFD (r)" = "#2196F3",
  "PFD (3)" = "#2196F3",
  "PFD (10)" = "#2196F3",
  "PFD (100)" = "#2196F3",
  "PFD (1000)" = "#2196F3",
  "KFD" = "#2196F3",
  "SFD" = "#2196F3",
  "SDAFD" = "#2196F3",
  "NLDFD" = "#2196F3",
  "PSDFD (Voss1998)" = "#2196F3",
  "PSDFD (Hasselman2013)" = "#2196F3",
  "HFD" = "#2196F3",
  "SVDEn" = "#E91E63",
  "K2En" = "#E91E63",
  "AttEn" = "#E91E63",
  "PhasEn (4)" = "#E91E63",
  "PhasEn (8)" = "#E91E63",
  "GridEn (3)" = "#E91E63",
  "GridEn (10)" = "#E91E63",
  "DiffEn" = "#E91E63",
  "DistrEn" = "#E91E63",
  "ApEn" = "#E91E63",
  "cApEn" = "#E91E63",
  "PEn" = "#E91E63",
  "WPEn" = "#E91E63",
  "SampEn" = "#E91E63",
  "FuzzyEn" = "#E91E63",
  "FuzzyApEn" = "#E91E63",
  "FuzzycApEn" = "#E91E63",
  "MSEn" = "#E91E63",
  "CMSEn" = "#E91E63",
  "RCMSEn" = "#E91E63",
  "MMSEn" = "#E91E63",
  "IMSEn" = "#E91E63",
  "MSApEn" = "#E91E63",
  "MSPEn" = "#E91E63",
  "CMSPEn" = "#E91E63",
  "MMSPEn" = "#E91E63",
  "IMSPEn" = "#E91E63",
  "MSWPEn" = "#E91E63",
  "CMSWPEn" = "#E91E63",
  "MMSWPEn" = "#E91E63",
  "IMSWPEn" = "#E91E63",
  "CPEn" = "#E91E63",
  "CWPEn" = "#E91E63",
  "CRPEn" = "#E91E63",
  "BubbEn" = "#E91E63",
  "CoSiEn" = "#E91E63",
  "MSCoSiEn" = "#E91E63",
  "IncrEn" = "#E91E63",
  "MSIncrEn" = "#E91E63",
  "SlopEn" = "#E91E63",
  "SlopEn (7)" = "#E91E63",
  "MSSlopEn" = "#E91E63",
  "SyDyEn" = "#E91E63",
  "MSSyDyEn" = "#E91E63",
  "MMSyDyEn" = "#E91E63",
  "DispEn" = "#E91E63",
  "DispEn (fluctuation)" = "#E91E63",
  "FuzzyMSEn" = "#E91E63",
  "FuzzyCMSEn" = "#E91E63",
  "FuzzyRCMSEn" = "#E91E63",
  "FuzzyMMSEn" = "#E91E63",
  "FuzzyIMSEn" = "#E91E63",
  "ShanEn (A)" = "#E91E63",
  "ShanEn (B)" = "#E91E63",
  "ShanEn (C)" = "#E91E63",
  "ShanEn (D)" = "#E91E63",
  "ShanEn (r)" = "#E91E63",
  "ShanEn (3)" = "#E91E63",
  "ShanEn (10)" = "#E91E63",
  "ShanEn (100)" = "#E91E63",
  "ShanEn (1000)" = "#E91E63",
  "CREn (A)" = "#E91E63",
  "CREn (B)" = "#E91E63",
  "CREn (C)" = "#E91E63",
  "CREn (D)" = "#E91E63",
  "CREn (r)" = "#E91E63",
  "CREn (3)" = "#E91E63",
  "CREn (10)" = "#E91E63",
  "CREn (100)" = "#E91E63",
  "CREn (1000)" = "#E91E63",
  "EnofEn (3)" = "#E91E63",
  "EnofEn (5)" = "#E91E63",
  "EnofEn (9)" = "#E91E63",
  "RangeEn" = "#E91E63",
  "SPEn (10)" = "#E91E63",
  "SPEn (50)" = "#E91E63",
  "SPEn (100)" = "#E91E63",
  "HEn" = "#E91E63",
  "KLEn" = "#E91E63",
  "KLEn (corrected)" = "#E91E63",
  "H" = "#2196F3",
  "LZC" = "#2196F3",
  "PLZC" = "#2196F3",
  "MSLZC" = "#2196F3",
  "MSPLZC" = "#2196F3",
  "RR" = "#2196F3",
  "FI" = "#FF5722",
  "FSI" = "#FF5722",
  "PowEn" = "#FF5722",
  "CD" = "#FF5722",
  "Hjorth" = "#FF5722",
  "LLE" = "#2196F3",
  "RQA_RecurrenceRate" = "#4CAF50",
  "RQA_Determinism" = "#4CAF50",
  "RQA_Laminarity" = "#4CAF50",
  "RQA_TrappingTime" = "#4CAF50",
  "RQA_DeteRec" = "#4CAF50",
  "RQA_Divergence" = "#4CAF50",
  "RQA_LamiDet" = "#4CAF50",
  "RQA_RecurrenceRate" = "#4CAF50",
  "RQA_L" = "#4CAF50",
  "RQA_LEn" = "#4CAF50",
  "RQA_VMax" = "#4CAF50",
  "RQA_VEn" = "#4CAF50",
  "RQA_W" = "#4CAF50",
  "RQA_WMax" = "#4CAF50",
  "RQA_WEn" = "#4CAF50",
  "DFA" = "#4CAF50",
  "MFDFA_Fluctuation" = "#4CAF50",
  "MFDFA_Width" = "#4CAF50",
  "MFDFA_Peak" = "#4CAF50",
  "MFDFA_Mean" = "#4CAF50",
  "MFDFA_Max" = "#4CAF50",
  "MFDFA_Delta" = "#4CAF50",
  "MFDFA_Asymmetry" = "#4CAF50",
  "MFDFA_Increment" = "#4CAF50"
)

# length(unique(df$Index))
# unique(df$Index)[!unique(df$Index) %in% names(colors)]
# names(colors)[!names(colors) %in% unique(df$Index)]
```

### Computation Time

```{r computation_time, message=FALSE, warning=FALSE, fig.width=16*1.25, fig.height=10*1.25, cache=FALSE, echo=FALSE, fig.cap="Median computation time difference between the different complexity indices algorithms.", cache=FALSE}
order <- df |>
  group_by(Index) |>
  summarize(Duration = median(Duration)) |>
  arrange(Duration) |>
  mutate(Index = factor(Index, levels = Index))

df <- mutate(df, Index = fct_relevel(Index, as.character(order$Index)))

df |>
  filter(!Index %in% c("SD", "Length", "Noise", "Random", "Frequency")) |>
  mutate(Duration = Duration * 10000) |>
  ggplot(aes(x = Index, y = Duration)) +
  # geom_violin(aes(fill = Index)) +
  geom_hline(yintercept = 10**seq(0, 5, by = 2), linetype = "dotted", color = "#9E9E9E") +
  geom_hline(yintercept = 10**seq(1, 5, by = 2), color = "#9E9E9E") +
  ggdist::stat_slab(side = "bottom", aes(fill = Index), adjust = 3) +
  ggdist::stat_dotsinterval(aes(fill = Index, slab_size = NA)) +
  theme_modern() +
  scale_y_log10(breaks = 10**seq(0, 5), labels = function(x) sprintf("%g", x)) +
  scale_fill_manual(values = colors, guide = "none") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  labs(x = NULL, y = "Computation Time")
```



```{r time1, message=FALSE, warning=FALSE, include=FALSE, eval=FALSE}
dfsummary <- df |>
  filter(!Index %in% c("SD", "Length", "Noise", "Random", "Frequency")) |>
  mutate(Duration = Duration * 10000) |>
  group_by(Index, Length) |>
  summarize(
    CI_low = median(Duration) - sd(Duration),
    CI_high = median(Duration) + sd(Duration),
    Duration = median(Duration)
  )
dfsummary$CI_low[dfsummary$CI_low < 0] <- 0


dfsummary |>
  ggplot(aes(x = Index, y = Duration)) +
  # geom_hline(yintercept = c(0.001, 0.01, 0.1, 1), linetype = "dotted") +
  geom_hline(yintercept = 10**seq(0, 5, by = 2), linetype = "dotted", color = "#9E9E9E") +
  geom_line(aes(group = Length, color = Length)) +
  geom_hline(yintercept = 10**seq(1, 5, by = 2), color = "#9E9E9E") +
  # geom_point(aes(color = Length)) +
  theme_modern() +
  scale_y_log10(breaks = 10**seq(0, 5), labels = function(x) sprintf("%g", x)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  guides(alpha = "none") +
  labs(y = "Time to compute", x = NULL, color = "Signal length")
```


```{r time2, message=FALSE, warning=FALSE, include=FALSE, eval=FALSE}
df |>
  filter(!Index %in% c("SD", "Length", "Noise", "Random", "Frequency")) |>
  mutate(Duration = Duration * 10000) |>
  ggplot(aes(x = as.factor(Length), y = Duration)) +
  # geom_hline(yintercept = c(0.001, 0.01, 0.1, 1), linetype = "dotted") +
  geom_line(data = dfsummary, aes(group = 1)) +
  geom_violin(aes(fill = Length)) +
  facet_wrap(~Index) +
  scale_y_log10(breaks = 10**seq(0, 4), labels = function(x) sprintf("%g", x)) +
  scale_fill_viridis_c(guide = "none") +
  theme_modern() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
```

Despite the relative shortness of the signals considered (a few thousand points at most), the fully-parallelized data generation script took 12h to run on a 48-cores machine. After summarizing and sorting the indices by computation time, the most striking feature are the orders of magnitude of difference between the fastest and slowest indices. Some of them are also particularly sensitive to the data length, a property which combined with computational expensiveness leads to indices being 100,000 slower to compute than other basic metrics.

Multiscale indices are among the slowest, due to their iterative nature (a given index is computed multiple times on coarse-grained subseries of the signal). Indices related to Recurrence Quantification Analysis (RQA) are also relatively slow and don't scale well with signal length.


```{r message=FALSE, warning=FALSE, results='hide', echo=FALSE}
# Show and filter out NaNs
as.character(df[is.na(df$Result), "Index"])
df <- filter(df, !is.na(Result))

as.character(df[is.infinite(df$Result), "Index"])
df <- filter(df, !is.infinite(Result))

df <- df |>
  group_by(Index) |>
  standardize(select = "Result") |>
  ungroup()
```


### Duplicates

```{r message=FALSE, warning=FALSE, fig.width=16, fig.height=15, cache=FALSE}
data <- df |>
  mutate(i = paste(Signal, Length, Noise_Type, Noise_Intensity, sep = "__")) |>
  select(i, Index, Result) |>
  pivot_wider(names_from = "Index", values_from = "Result") |>
  select(-i)

# pca <- principal_components(data, n=1) |>
#   arrange(desc(sign(PC1)), desc(abs(PC1)))

get_cor <- function(data, plot=FALSE) {
  cor <- correlation::correlation(data, method = "pearson", redundant = TRUE) |>
    correlation::cor_sort(hclust_method = "ward.D2")

  if(plot) {
    p_data <- cor |>
      cor_lower() |>
      mutate(
        Text = insight::format_value(r, zap_small = TRUE, digits = 3),
        Text = str_replace(str_remove(Text, "^0+"), "^-0+", "-"),
        Parameter2 = fct_rev(Parameter2)
      )

    p <- p_data |>
      ggplot(aes(x = Parameter2, y = Parameter1)) +
      geom_tile(aes(fill = r)) +
      # geom_text(aes(label = Text), size = 2) +
      scale_fill_gradient2(low = "#2196F3", mid = "white", high = "#F44336", midpoint = 0, limit = c(-1, 1), space = "Lab", name = "Correlation", guide = "legend") +
      scale_x_discrete(expand = c(0, 0)) +
      scale_y_discrete(expand = c(0, 0)) +
      labs(title = "Correlation Matrix of Complexity Indices", x = NULL, y = NULL) +
      theme_minimal() +
      theme(
        axis.text.x = element_text(angle = 90, hjust = 1),
        plot.title = element_text(hjust = 0.5),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()
      )
    plot(p)
  }
  cor
}

cor <- get_cor(data)
```

```{r eval=TRUE, message=FALSE, warning=FALSE, include=FALSE}
cor |>
  cor_lower() |>
  filter(Parameter1 %in% names(data), Parameter2 %in% names(data)) |>
  arrange(desc(abs(r)), Parameter1) |>
  filter(Parameter1 != Parameter2) |>
  filter(abs(r) > .97) |>
  select(Parameter1, Parameter2, r)
```


We removed statistically redundant indices, such as *PowEn* (identical to *SD*), *CREn (100)*  (identical to *CREn (10)*), and *FuzzyRCMSEn* (identical to *RCMSEn*).




```{r eval=FALSE, message=FALSE, warning=FALSE, cache=FALSE, include=FALSE}
# Duplicates
# ===========
averagetime <- arrange(summarize(group_by(df, Index), Duration = mean(Duration)), Duration)

filter(averagetime, Index %in% c("CREn (D)", "PFD (D)", "ShanEn (D)"))
filter(averagetime, Index %in% c("ShanEn (B)", "CREn (B)"))
filter(averagetime, Index %in% c("ShanEn (r)", "PFD (r)", "CREn (r)"))
filter(averagetime, Index %in% c("ShanEn (C)", "PFD (C)", "CREn (C)"))
filter(averagetime, Index %in% c("CREn (10)", "CREn (100)"))
filter(averagetime, Index %in% c("SVDEn", "FI"))
filter(averagetime, Index %in% c("PSDFD (Hasselman2013)", "PSDFD (Voss1998)"))
filter(averagetime, Index %in% c("MMSEn", "IMSEn"))
filter(averagetime, Index %in% c("H (corrected)", "H (uncorrected)"))
filter(averagetime, Index %in% c("FuzzyEn", "FuzzyApEn"))
filter(averagetime, Index %in% c("RCMSEn", "FuzzyRCMSEn"))
filter(averagetime, Index %in% c("SVDEn", "FuzzycApEn"))
filter(averagetime, Index %in% c("CPEn", "CRPEn"))
filter(averagetime, Index %in% c("NLDFD", "RR"))
```

### Correlation

```{r correlation, message=FALSE, warning=FALSE, cache=FALSE, fig.width=16, fig.height=15}
data <- data |>
  select(
    -`FuzzyRCMSEn`,
    -`CREn (100)`,
    -`PowEn`
  )


cor <- get_cor(data, plot=TRUE)
```

The Pearson correlation analysis revealed that complexity indices, despite their multitude, their unicities and specificities, do indeed share similarities. They form clusters, with two major ones easily appearing to the naked eye (the blue and the red groups). These two anti-correlated groups are driven by the fact that some indices, by design, index the "predictability", whereas other the "randomness", and thus are negatively related to one-another.

### Factor Analysis

```{r message=FALSE, warning=FALSE, fig.width=14, fig.height=7}
r <- correlation::cor_smooth(as.matrix(cor))

plot(parameters::n_factors(data, cor = r, n_max=20))
# plot(parameters::n_components(data, cor = r))
```


```{r loadings, message=FALSE, warning=FALSE, fig.width=12, fig.height=18}
rez <- parameters::factor_analysis(data, cor = r, n = 14, rotation = "varimax", sort = TRUE, fm="mle")
# rez <- parameters::principal_components(data, n = 15, sort = TRUE)
# rez

col <- gsub('[[:digit:]]+', '', names(rez)[2])
closest <- colnames(select(rez, starts_with(col)))[apply(select(rez, starts_with(col)), 1, \(x) which.max(abs(x)))]


loadings <- attributes(rez)$loadings_long |>
  mutate(
    Loading = Loading,
    Component = fct_relevel(Component, rev(names(select(rez, starts_with(col))))),
    Variable = fct_rev(fct_relevel(Variable, rez$Variable))
  )

colors <- setNames(see::palette_material("rainbow")(length(levels(loadings$Component))), levels(loadings$Component))

# Sort by sign too
names(closest) <- rev(levels(loadings$Variable))

idx_order <- loadings |>
  mutate(Closest = closest[as.character(loadings$Variable)],
         Sign = sign(Loading)) |>
  filter(Component == Closest) |>
  arrange(desc(Component), desc(Sign), desc(abs(Loading))) |>
  pull(Variable) |>
  as.character()

separations <- table(closest)[intersect(levels(loadings$Component), unique(closest))]

selection <- c("ShanEn (D)",
               "NLDFD",
               "SVDEn",
               "AttEn",
               "PSDFD",
               "MFDFA_Mean",
               "FuzzyMSEn",
               "MSWPEn",
               "MFDFA_Increment",
               "ShanEn (r)",
               "RQA_ReccurenceRate",
               "WPEn")
face <- rep("plain", length(idx_order))
face[idx_order %in% c("SD", "Length", "Noise", "Random", "Frequency")] <- "italic"
face[idx_order %in% selection] <- "bold"


p1 <- loadings |>
  mutate(Variable = fct_relevel(Variable, rev(idx_order))) |>
  # filter(Variable == "CD") |>
  ggplot(aes(x = Variable, y = Loading)) +
  geom_bar(aes(fill = Component), stat = "identity") +
  geom_vline(xintercept = c("SD", "Length", "Noise", "Random"), color = "red") +
  geom_vline(xintercept = head(cumsum(separations), -1) + 0.5) +
  scale_y_continuous(expand = c(0, 0)) +
  scale_fill_material_d("rainbow") +
  coord_flip() +
  theme_minimal() +
  guides(fill = guide_legend(reverse = TRUE)) +
  labs(x = NULL) +
  theme(
    axis.text.y = element_text(
      color = rev(colors[closest]),
      face = rev(face),
      hjust = 0.5
    ),
    axis.text.x = element_blank(),
    plot.title = element_text(hjust = 0.5),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  )

p2 <- order |>
  mutate(Duration = 1 + Duration * 10000) |>
  filter(Index %in% loadings$Variable) |>
  mutate(Index = fct_relevel(Index, rev(idx_order)),
         Duration = ifelse(is.na(Duration), 0, Duration)) |>
  ggplot(aes(x = log10(Duration), y = Index)) +
  geom_bar(aes(fill = log10(Duration)), stat = "identity") +
  geom_hline(yintercept = head(cumsum(separations), -1) + 0.5) +
  scale_x_reverse(expand = c(0, 0)) +
  scale_y_discrete(position = "right") +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Computation Time", y = NULL) +
  theme_minimal() +
  theme(
    axis.text.y = element_blank(),
    axis.text.x = element_blank(),
    plot.title = element_text(hjust = 0.5),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  )

(p2 | p1) + patchwork::plot_annotation(title = "Computation Time and Factor Loading", theme = theme(plot.title = element_text(hjust = 0.5, face = "bold")))
```

The aggreement method suggested that the 125 indices can be mapped on a multidimensional space of 14 orthogonal latent factors, that we extracted using a *varimax* rotation for ease of interpretation. We then took interest in the loading profile of each indices, and in particular the latent dimension that maximally related to each index.

The first factor is the closest to the largest amount of indices. Many indices with positive and strong loadings are particularly sensitive to the deviation of consecutive differences (e.g., *ShanEn - D*, *NLDFD*, *PFD - D*). It was negatively loaded by indices related to Detrended Fluctuation Analysis (DFA), which tend to index the presence of long-term correlations. This latent factor might encapsulate the predominance of short-term vs. long-term unpredictability. Indices that are the most representative (positively and negatively) and have a relatively low computational cost include *ShanEn - D*, *NLDFD*, *PFD - D*, and *AttEn*, *PSDFD*, *FuzzyMSEn*.

The second factor was loaded maximally by signal *length* and *SD*, and thus might not capture features of complexity *per se*. Indices the most related to it were indices known to be sensitive to signal length, such as *ApEn*.

The third factor included multiscale indices, such as *MSWPEn*.

The fourth factor included indices that quantified the diversity of the tendency of a signal to revisit a past state (within a certain tolerance threshold). It was positively loaded by *ShanEn - r* and negatively by *RQA - Reccurence Rate*.

The fifth factor was loaded by permutation entropy indices, such as *WPEn*.

The sixth factor was driven by indices that were based on converting the signal into a number of bins.

The seventh factor was loaded positively by the amount of noise, and negatively by multifractal indices such as *MFDFA - Increment*, suggesting a sensitivity to regularity.

The last notable result is that indices based on a symbolization (discretization) of the time series do tend to create factors alongside the symbolization method.

Finally, as a manipulation check of our factorization method, the random vector does indeed form its own factor, and doesn't load unto anything else.




### Correlation Network

For illustration purposes, we also represented the correlation matrix as a connectivity graph.

```{r ggm, message=FALSE, warning=FALSE, fig.width=13, fig.height=13}
library(ggraph)

g <- cor |>
  cor_lower() |>
  mutate(width = abs(r),
         edgecolor = as.character(sign(r))) |>
  filter(!Parameter1 %in% c("SD", "Length", "Random", "Frequency", "Noise"),
         !Parameter2 %in% c("SD", "Length", "Random", "Frequency", "Noise")) |>
  tidygraph::as_tbl_graph(directed=FALSE) |>
  mutate(importance = tidygraph::centrality_authority(weights = abs(r)),
         group = as.factor(tidygraph::group_louvain(weights = abs(r)))) |>
  tidygraph::activate("edges") |>
  filter(abs(r) > 0.6) |>
  tidygraph::activate("nodes") |>
  filter(!tidygraph::node_is_isolated()) |>
  mutate(colors = closest[name],
         selection = ifelse(name %in% selection, TRUE, FALSE))


p1 <- g |>
  ggraph(layout = 'linear', circular = TRUE) +  # fr # lgl # drl # kk
  ggraph::geom_edge_arc(aes(edge_width=width, edge_colour=edgecolor), alpha=0.66, strength=0.3) +
  # ggraph::geom_conn_bundle(aes(edge_width=width, edge_colour=edgecolor), alpha=0.66) +
  ggraph::geom_node_point(aes(size = importance)) +
  ggraph::geom_node_text(aes(x = x*1.05, y=y*1.05, label = name, angle = -((-node_angle(x, y)+90)%%180)+90, hjust='outward', color=selection)) +
  scale_edge_color_manual(values = c("1" = "#2E7D32", "-1"="#C62828"), guide = "none") +
  scale_edge_width_continuous(range = c(0.005, 0.66), guide = "none") +
  scale_size_continuous(range = c(0.1, 2), guide = "none") +
  scale_fill_material_d(guide= "none") +
  scale_colour_manual(values=c("TRUE" = "red", "FALSE"="black"), guide= "none") +
  ggtitle("Correlation Network") +
  ggraph::theme_graph() +
  theme(plot.title = element_text(face="bold", hjust=0.5)) +
  expand_limits(x = c(-1.25, 1.25), y = c(-1.25, 1.25))

p1
```


### Hierarchical Clustering

We ran hierarchical clustering (with a Ward D2 distance) to provide additional information or confirmation about the groups discussed above. This allowed us to fine-grain our recommendations of complimentary complexity indices.

```{r graph, message=FALSE, warning=FALSE, fig.width=13.4, fig.height=13.4}
clust <- data |>
  select(-SD, -Length, -Random, -Frequency, -Noise) |>
  t() |>
  as.data.frame() |>
  dist() |>
  hclust(method = "ward.D2")


clusters <- cutree(clust, h = 150)
colors <- c("red", "black", see::palette_material("rainbow")(max(clusters)))
names(colors) <- c("TRUE", "FALSE", seq(1:max(clusters)))

p2 <- clust |>
  create_layout(layout = 'dendrogram', circular = TRUE, repel=TRUE) |>
  attr("graph") |>
  tidygraph::activate("nodes") |>
  mutate(colors = closest[label],
         selection = ifelse(label %in% selection, TRUE, FALSE),
         cluster = as.factor(clusters[label])) |>
  ggraph(layout = 'dendrogram', circular = TRUE, repel=TRUE) +
  geom_edge_elbow() +
  geom_node_point(aes(filter=leaf, color = cluster), size=3) +
  geom_node_text(aes(x = x*1.05, y=y*1.05, label = label, angle = -((-node_angle(x, y)+90)%%180)+90, hjust='outward', color = selection)) +
  scale_colour_manual(values=colors, guide= "none") +
  coord_fixed() +
  ggtitle("Hierarchical Clustering") +
  theme_graph() +
  theme(plot.title = element_text(face="bold", hjust=0.5)) +
  expand_limits(x = c(-1.25, 1.25), y = c(-1.25, 1.25))

p2
```




### Visualization

Finally, we visualized the expected value of our selection of indices for different types of signals under different conditions of noise.

```{r message=FALSE, warning=FALSE, fig.width=14, fig.height=7}
model <- mgcv::gam(Result ~ s(Noise_Intensity, by = interaction(Index, Signal)),
            data=df |>
              filter(Index %in% selection) |>
              mutate(Noise_Type = as.factor(Noise_Type)))

estimate_means(model, at = c("Index", "Signal", "Noise_Intensity")) |>
  ggplot(aes(y = Mean, x = Noise_Intensity)) +
  geom_line(aes(color = Signal), size=1) +
  facet_wrap(~Index) +
  scale_linetype_manual(values = c("-2" = 3, "-1" = 4, "0" = 2, "1" = 5, "2" = 1)) +
  theme_classic() +
  scale_color_material("rainbow") +
  theme(panel.grid.major = element_line(colour = "#EEEEEE"),
        strip.background = element_blank()) +
  labs(y = "Standardized Index Value", x = "Noise Intensity", color = "Signal Type")
```

## Discussion

As complexity science grows in size and application, a systematic approach to compare their "performance" becomes necessary to increase the clarity and structure of the field. The word *performance* is here to be understood in a relative sense, as any such endeavor faces the "hard problem" of complexity science. The indices are sensitive to specific objective properties of a signal that we consider part of over-arching concepts such as "complex" and "chaotic". But it is unclear how these high-level concepts transfer back, in a top-down fashion, into a combination of lower-level features, such as short-term vs. long-term variability, auto-correlation, information, randomness, and so on. As such, it is conceptually complicated to benchmark complexity measures against "objectively" complex vs. non-complex signals. In other words, we know that different characteristics can contribute to the "complexity" of a signal, but there is not a one-to-one correspondence between the latter and the former.

Hence the choice of the current paradigm, in which we generated different types of signals to which we systematically added different types and amount of perturbations. However, we did not seek at measuring how complexity indices can discriminate between these features or systems, nor did we attempt at mimicking real-life signals or scenarios. The goal was instead to generate enough variability to reliably map the relationships between the indices.

The plurality of underlying components of empirical complexity (what is measured by complexity indices) seems to be somewhat confirmed by our results, showing that complexity indices are more or less sensitive to various orthogonal latent dimensions. One of the limitation of the current study has to do with the limited possibilities of interpretation of these underlying dimensions, and future studies are needed to discuss them.

Indices that were highlighted as encapsulating information about different underlying dimensions at a relatively low computational cost include `r datawizard::text_concatenate(paste0("*", selection, "*"))`. These indices might be complimentary in offering a comprehensive profile of the complexity of a time series.

## References
