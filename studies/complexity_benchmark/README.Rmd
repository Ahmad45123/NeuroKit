---
output:
  github_document:
    toc: false
    fig_width: 10.08
    fig_height: 6
tags: [r, complexity]
vignette: >
  %\VignetteIndexEntry{README}
  \usepackage[utf8]{inputenc}
  %\VignetteEngine{knitr::rmarkdown}
editor_options:
  chunk_output_type: console
bibliography: bibliography.bib
csl: utils/apa.csl
---

```{r, echo = FALSE, warning=FALSE, message=FALSE}
# options and parameters
options(digits = 3)

knitr::opts_chunk$set(
  collapse = TRUE,
  dpi = 450,
  fig.path = "../../studies/complexity_benchmark/figures/"
)
```



<!-- # Benchmarking and Analysis of Complexity Measures -->
<!-- # Measuring Chaos: Complexity and Fractal Physiology using NeuroKit2 -->
<!-- # Measuring Chaos with NeuroKit2: An Empirical Comparison of Fractal Physiology Complexity Indices -->
# The Structure of Chaos: An Empirical Comparison of Fractal Physiology Complexity Indices using NeuroKit2

*This study can be referenced by* [*citing the package and the documentation*](https://neuropsychology.github.io/NeuroKit/cite_us.html).

**We'd like to improve this study, but unfortunately we currently don't have the time. If you want to help to make it happen, please contact us!**

## Introduction

Complexity is an umbrella term for concepts derived from information theory, chaos theory, and fractal mathematics, used to quantify unpredictability, entropy, and/or randomness. Using these tools to characterize signals [a subfield commonly referred to as "fractal physiology", @bassingthwaighte2013fractal] has shown promising results in physiology in the assessment and diagnostic of the state and health of living systems [@lau2021brain, @ehlers1995chaos].

There has been a large and accelerating increase in the number of complexity indices in the past few decades. These new procedures are usually mathematically well-defined and theoretically promising. However, few empirical evidence exist to understand their differences and similarities. Moreover, some can be very expensive in terms of computation power and thus, time, which can become an issue in some applications such as high sampling-rate techniques (e.g., M/EEG) or real-time settings (brain-computer interface). As such, having a general view depicting the relationship between the indices with information about their computation time would be useful, for instance to guide the indices selection in settings where time or computational power is limited. 

One of the contributing factor of this lack of empirical comparison is the lack of free, open-source, unified, and easy to use software for computing various complexity indices. Indeed, most of them are described mathematically in journal articles, and reusable code is seldom made available, which limits their further application and validation. *NeuroKit2* [@Makowski2021neurokit] is a Python package for physiological signal processing that aims at providing the most comprehensive, accurate and fast pure Python implementations of complexity indices. 

Leveraging this tool, the goal of this study is to empirically compare a vast number of complexity indices, inspect how they relate to one another, and extract some recommendations for indices selection, based on their added-value and computational efficiency. Using NeuroKit2, we will compute more than a hundred complexity indices on various types of signals, with varying degrees of noise. We will then project the results on a latent space through factor analysis, and report the most interesting indices in regards to their representation of the latent dimensions.


## Methods

```{r fig1_signals, message=FALSE, warning=FALSE, fig.height=16, fig.width=10, echo=FALSE, fig.cap="Different types of simulated signals, to which was added 5 types of noise (violet, blue, white, pink, and brown) with different intensities. For each signal type, the first row shows the signal with a minimal amount of noise, and the last with a maximal amount of noise. We can see that adding Brown noise turns the signal into a Random-walk (i.e., a Brownian motion)."}
library(tidyverse)
library(easystats)
library(patchwork)

df <- read.csv("data_Signals.csv") |>
  mutate(
    Method = as.factor(Method),
    Noise = as.factor(Noise),
    Noise = fct_recode(Noise, "Violet" = "-2", "Blue" = "-1", "White" = "0", "Pink" = "1", "Brown" = "2"),
    Intensity = as.factor(insight::format_value(Noise_Intensity))
  )

df <- df |>
  filter(Intensity %in% levels(df$Intensity)[c(1, round(length(levels(df$Intensity)) / 3), length(levels(df$Intensity)))])

make_plot <- function(method = "Random-Walk", title = "Random-Walk", color = "red") {
  df |>
    filter(Method == method) |>
    ggplot(aes(x = Duration, y = Signal)) +
    geom_line(color = color, size=0.3) +
    ggside::geom_ysidedensity(aes(x = stat(density))) +
    facet_grid(Intensity ~ Noise, labeller = label_value) +
    labs(y = NULL, title = title, x = NULL) +
    theme_minimal() +
    theme(
      axis.ticks = element_blank(),
      axis.text = element_blank(),
      plot.title = element_text(hjust = 0.5),
      ggside.panel.border = element_blank(),
      ggside.panel.grid = element_blank(),
      ggside.panel.background = element_blank()
    )
}

p1 <- make_plot(method = "Random-Walk", title = "Random-Walk", color = "#795548")
p2 <- make_plot(method = "lorenz_10_2.5_28", title = "Lorenz (\u03c3=10, \u03B2=2.5, \u03C1=28)", color = "#FF5722")
p3 <- make_plot(method = "lorenz_20_2_30", title = "Lorenz (\u03c3=20, \u03B2=2, \u03C1=30)", color = "#E91E63")
p4 <- make_plot(method = "oscillatory", title = "Oscillatory", color = "#2196F3")
p5 <- make_plot(method = "fractal", title = "Fractal", color = "#4CAF50")

p1 / p2 / p3 / p4 / p5 + patchwork::plot_annotation(title = "Examples of Simulated Signals", theme = theme(plot.title = element_text(face = "bold", hjust = 0.5)))
```

The script to generate the data can be found at ...

We started by generating 5 types of signals, one random-walk, two oscillatory signals made (one made of harmonic frequencies that results in a self-repeating - fractal-like - signal), and two complex signals derived from Lorenz systems (with parameters ($\sigma = 10, \beta = 2.5, \rho = 28$); and ($\sigma = 20, \beta = 2, \rho = 30$), respectively). Each of this signal was iteratively generated at ... different lengths (). The resulting vectors were standardized and each were added 5 types of $(1/f)^\beta$ noise (namely violet $\beta=-2$, blue $\beta=-1$, white $\beta=0$, pink $\beta=1$, and brown $\beta=2$ noise). Each noise type was added at ... different intensities (linearly ranging from 0.1 to 4). Examples of generated signals are presented in **Figure 1**.

The combination of these parameters resulted in a total of `r 32*5*5*6` signal iterations. For each of them, we computed ... indices, and additionally basic metric such as the standard deviation (*SD*), the *length* of the signal and its mean *frequency*. The parameters used (such as the time-delay $\tau$ or the embedding dimension) are documented in the data generation script. For a complete description of the various indices used, please refer to NeuroKit's documentation (https://neuropsychology.github.io/NeuroKit).


## Results

The data analysis script, the data and the code for the figures is fully available at ...

```{r message=FALSE, warning=FALSE, results='hide', echo=FALSE}
df <- read.csv("data_Complexity.csv") |>
  mutate(Method = as.factor(Method))
```

```{r message=FALSE, warning=FALSE, include=FALSE}
colors <- c(
  "SD" = "red",
  "Noise" = "red",
  "Length" = "red",
  "Random" = "red",
  "Frequency" = "red",
  "PFD (A)" = "#2196F3",
  "PFD (B)" = "#2196F3",
  "PFD (C)" = "#2196F3",
  "PFD (D)" = "#2196F3",
  "PFD (r)" = "#2196F3",
  "PFD (3)" = "#2196F3",
  "PFD (10)" = "#2196F3",
  "PFD (100)" = "#2196F3",
  "PFD (1000)" = "#2196F3",
  "KFD" = "#2196F3",
  "SFD" = "#2196F3",
  "SDAFD" = "#2196F3",
  "NLDFD" = "#2196F3",
  "PSDFD (Voss1998)" = "#2196F3",
  "PSDFD (Hasselman2013)" = "#2196F3",
  "HFD" = "#2196F3",
  "SVDEn" = "#E91E63",
  "K2En" = "#E91E63",
  "AttEn" = "#E91E63",
  "PhasEn (4)" = "#E91E63",
  "PhasEn (8)" = "#E91E63",
  "GridEn (3)" = "#E91E63",
  "GridEn (10)" = "#E91E63",
  "DiffEn" = "#E91E63",
  "DistrEn" = "#E91E63",
  "ApEn" = "#E91E63",
  "cApEn" = "#E91E63",
  "PEn" = "#E91E63",
  "WPEn" = "#E91E63",
  "SampEn" = "#E91E63",
  "FuzzyEn" = "#E91E63",
  "FuzzyApEn" = "#E91E63",
  "FuzzycApEn" = "#E91E63",
  "MSEn" = "#E91E63",
  "CMSEn" = "#E91E63",
  "RCMSEn" = "#E91E63",
  "MMSEn" = "#E91E63",
  "IMSEn" = "#E91E63",
  "MSApEn" = "#E91E63",
  "MSPEn" = "#E91E63",
  "CMSPEn" = "#E91E63",
  "MMSPEn" = "#E91E63",
  "IMSPEn" = "#E91E63",
  "MSWPEn" = "#E91E63",
  "CMSWPEn" = "#E91E63",
  "MMSWPEn" = "#E91E63",
  "IMSWPEn" = "#E91E63",
  "CPEn" = "#E91E63",
  "CWPEn" = "#E91E63",
  "CRPEn" = "#E91E63",
  "BubbEn" = "#E91E63",
  "CoSiEn" = "#E91E63",
  "MSCoSiEn" = "#E91E63",
  "IncrEn" = "#E91E63",
  "MSIncrEn" = "#E91E63",
  "SlopEn" = "#E91E63",
  "SlopEn (7)" = "#E91E63",
  "MSSlopEn" = "#E91E63",
  "SyDyEn" = "#E91E63",
  "MSSyDyEn" = "#E91E63",
  "MMSyDyEn" = "#E91E63",
  "DispEn" = "#E91E63",
  "DispEn (fluctuation)" = "#E91E63",
  "FuzzyMSEn" = "#E91E63",
  "FuzzyCMSEn" = "#E91E63",
  "FuzzyRCMSEn" = "#E91E63",
  "FuzzyMMSEn" = "#E91E63",
  "FuzzyIMSEn" = "#E91E63",
  "ShanEn (A)" = "#E91E63",
  "ShanEn (B)" = "#E91E63",
  "ShanEn (C)" = "#E91E63",
  "ShanEn (D)" = "#E91E63",
  "ShanEn (r)" = "#E91E63",
  "ShanEn (3)" = "#E91E63",
  "ShanEn (10)" = "#E91E63",
  "ShanEn (100)" = "#E91E63",
  "ShanEn (1000)" = "#E91E63",
  "CREn (A)" = "#E91E63",
  "CREn (B)" = "#E91E63",
  "CREn (C)" = "#E91E63",
  "CREn (D)" = "#E91E63",
  "CREn (r)" = "#E91E63",
  "CREn (3)" = "#E91E63",
  "CREn (10)" = "#E91E63",
  "CREn (100)" = "#E91E63",
  "CREn (1000)" = "#E91E63",
  "EnofEn (3)" = "#E91E63",
  "EnofEn (5)" = "#E91E63",
  "EnofEn (9)" = "#E91E63",
  "RangeEn" = "#E91E63",
  "SPEn (10)" = "#E91E63",
  "SPEn (50)" = "#E91E63",
  "SPEn (100)" = "#E91E63",
  "HEn" = "#E91E63",
  "KLEn" = "#E91E63",
  "KLEn (corrected)" = "#E91E63",
  "H (corrected)" = "#2196F3",
  "H (uncorrected)" = "#2196F3",
  "LZC" = "#2196F3",
  "PLZC" = "#2196F3",
  "MSLZC" = "#2196F3",
  "MSPLZC" = "#2196F3",
  "RR" = "#2196F3",
  "FI" = "#FF5722",
  "FSI" = "#FF5722",
  "PowEn" = "#FF5722",
  "CD" = "#FF5722",
  "Hjorth" = "#FF5722",
  "LLE" = "#2196F3",
  "RQA_RecurrenceRate" = "#4CAF50",
  "RQA_Determinism" = "#4CAF50",
  "RQA_Laminarity" = "#4CAF50",
  "RQA_TrappingTime" = "#4CAF50",
  "RQA_Determinism_RecurrenceRate" = "#4CAF50",
  "RQA_Divergence" = "#4CAF50",
  "RQA_Laminarity_Determinism" = "#4CAF50",
  "RQA_RecurrenceRate" = "#4CAF50",
  "RQA_L" = "#4CAF50",
  "RQA_LEn" = "#4CAF50",
  "RQA_VMax" = "#4CAF50",
  "RQA_VEn" = "#4CAF50",
  "RQA_W" = "#4CAF50",
  "RQA_WMax" = "#4CAF50",
  "RQA_WEn" = "#4CAF50",
  "DFA" = "#4CAF50",
  "MFDFA_Fluctuation" = "#4CAF50",
  "MFDFA_Width" = "#4CAF50",
  "MFDFA_Peak" = "#4CAF50",
  "MFDFA_Mean" = "#4CAF50",
  "MFDFA_Max" = "#4CAF50",
  "MFDFA_Delta" = "#4CAF50",
  "MFDFA_Asymmetry" = "#4CAF50",
  "MFDFA_Increment" = "#4CAF50"
)

# unique(df$Index)[!unique(df$Index) %in% names(colors)]
# names(colors)[!names(colors) %in% unique(df$Index)]
```

### Computation Time

```{r computation_time, message=FALSE, warning=FALSE, fig.width=16*1.25, fig.height=10*1.25, cache=FALSE, echo=FALSE}
order <- df |>
  group_by(Index) |>
  summarize(Duration = median(Duration)) |>
  arrange(Duration) |>
  mutate(Index = factor(Index, levels = Index))

df <- mutate(df, Index = fct_relevel(Index, as.character(order$Index)))

df |>
  filter(!Index %in% c("SD", "Length", "Noise", "Random", "Frequency")) |>
  mutate(Duration = Duration * 10000) |>
  ggplot(aes(x = Index, y = Duration)) +
  # geom_violin(aes(fill = Index)) +
  geom_hline(yintercept = 10**seq(0, 5, by = 2), linetype = "dotted", color = "#9E9E9E") +
  geom_hline(yintercept = 10**seq(1, 5, by = 2), color = "#9E9E9E") +
  ggdist::stat_slab(side = "bottom", aes(fill = Index), adjust = 3) +
  ggdist::stat_dotsinterval(aes(fill = Index, slab_size = NA)) +
  theme_modern() +
  scale_y_log10(breaks = 10**seq(0, 5), labels = function(x) sprintf("%g", x)) +
  scale_fill_manual(values = colors, guide = "none") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  labs(x = NULL, y = "Computation Time")
```



```{r time1, message=FALSE, warning=FALSE, include=FALSE, eval=FALSE}
dfsummary <- df |>
  filter(!Index %in% c("SD", "Length", "Noise", "Random", "Frequency")) |>
  mutate(Duration = Duration * 10000) |>
  group_by(Index, Length) |>
  summarize(
    CI_low = median(Duration) - sd(Duration),
    CI_high = median(Duration) + sd(Duration),
    Duration = median(Duration)
  )
dfsummary$CI_low[dfsummary$CI_low < 0] <- 0


dfsummary |>
  ggplot(aes(x = Index, y = Duration)) +
  # geom_hline(yintercept = c(0.001, 0.01, 0.1, 1), linetype = "dotted") +
  geom_line(aes(group = Length, color = Length)) +
  # geom_point(aes(color = Length)) +
  theme_modern() +
  scale_y_log10(breaks = 10**seq(0, 4), labels = function(x) sprintf("%g", x)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  guides(alpha = "none") +
  labs(y = "Time to compute", x = NULL, color = "Signal length")
```


```{r time2, message=FALSE, warning=FALSE, include=FALSE, eval=FALSE}
df |>
  filter(!Index %in% c("SD", "Length", "Noise", "Random", "Frequency")) |>
  mutate(Duration = Duration * 10000) |>
  ggplot(aes(x = as.factor(Length), y = Duration)) +
  # geom_hline(yintercept = c(0.001, 0.01, 0.1, 1), linetype = "dotted") +
  geom_line(data = dfsummary, aes(group = 1)) +
  geom_violin(aes(fill = Length)) +
  facet_wrap(~Index) +
  scale_y_log10(breaks = 10**seq(0, 4), labels = function(x) sprintf("%g", x)) +
  scale_fill_viridis_c(guide = "none") +
  theme_modern() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
```

Despite the relative shortness of the signals considered (a few thousand points at most), the fully-parallelized data generation script took ...h to run on a 32-cores machine. After summarizing and sorting the indices by computation time, the most striking feature are the orders of magnitude of difference between the fastest and slowest indices. Some of them are also particularly sensitive to the data length, a property which combined with computational expensiveness leads to indices being 100,000 slower to compute than other basic metrics.

Multiscale indices are among the slowest, due to their iterative nature (a given index is computed multiple times on coarse-grained subseries of the signal). Indices related to Recurrence Quantification Analysis (RQA) are also slow and don't scale well with signal length.


```{r message=FALSE, warning=FALSE, results='hide', echo=FALSE}
# Show and filter out NaNs
as.character(df[is.na(df$Result), "Index"])
df <- filter(df, !is.na(Result))

as.character(df[is.infinite(df$Result), "Index"])
df <- filter(df, !is.infinite(Result))

df <- df |>
  group_by(Index) |>
  standardize(select = "Result") |>
  ungroup()
```


### Duplicates

```{r message=FALSE, warning=FALSE, fig.width=16, fig.height=15, cache=FALSE}
data <- df |>
  mutate(i = paste(Signal, Length, Noise_Type, Noise_Intensity, sep = "__")) |>
  select(i, Index, Result) |>
  pivot_wider(names_from = "Index", values_from = "Result") |>
  select(-i)

# pca <- principal_components(data, n=1) |>
#   arrange(desc(sign(PC1)), desc(abs(PC1)))

get_cor <- function(data, plot=FALSE) {
  cor <- correlation::correlation(data, method = "pearson", redundant = TRUE) |>
    correlation::cor_sort(hclust_method = "ward.D2")

  if(plot) {
    p_data <- cor |>
      cor_lower() |>
      mutate(
        Text = insight::format_value(r, zap_small = TRUE, digits = 3),
        Text = str_replace(str_remove(Text, "^0+"), "^-0+", "-"),
        Parameter2 = fct_rev(Parameter2)
      )

    p <- p_data |>
      ggplot(aes(x = Parameter2, y = Parameter1)) +
      geom_tile(aes(fill = r)) +
      # geom_text(aes(label = Text), size = 2) +
      scale_fill_gradient2(low = "#2196F3", mid = "white", high = "#F44336", midpoint = 0, limit = c(-1, 1), space = "Lab", name = "Correlation", guide = "legend") +
      scale_x_discrete(expand = c(0, 0)) +
      scale_y_discrete(expand = c(0, 0)) +
      labs(title = "Correlation Matrix of Complexity Indices", x = NULL, y = NULL) +
      theme_minimal() +
      theme(
        axis.text.x = element_text(angle = 90, hjust = 1),
        plot.title = element_text(hjust = 0.5),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()
      )
    plot(p)
  }
  cor
}

cor <- get_cor(data)
```

```{r eval=TRUE, message=FALSE, warning=FALSE, include=FALSE}
cor |>
  cor_lower() |>
  filter(Parameter1 %in% names(data), Parameter2 %in% names(data)) |>
  arrange(desc(abs(r)), Parameter1) |>
  filter(Parameter1 != Parameter2) |>
  filter(abs(r) > .97) |>
  select(Parameter1, Parameter2, r)
```


We will start by removing redundant indices. We removed *H (corrected)* (identical to *H (uncorrected)*) as it's slightly slower, *PowEn* (identical to *SD*), and *CREn (100)*  (identical to *CREn (10)*).

  
- **CREn (B)**, and **ShanEn (B)**
  - Remove *CREn (B)*  because it's slower.
- **CREn (D)**, **PFD (D)** and **ShanEn (D)**
  - Remove *CREn (D)* and *ShanEn (D)* because it's slower.
- **CREn (r)**, **PFD (r)** and **ShanEn (r)**
  - Remove *CREn (r)* and *ShanEn (r)* because it's slower.
- **PSDFD (Hasselman2013)** and **PSDFD (Voss1998)**
  - Remove **PSDFD (Voss1998)** because it's positively correlated with the rest.
- **RangeEn (A)**, **RangeEn (Ac)** and **RangeEn (B)**
  - Remove **RangeEn (A)**, **RangeEn (Ac)**  because they yield undefined entropies.
- **SVDEn**, and **FI**
  - Remove **FI**  because it's negatively correlated with the rest.
- **MMSEn**, and **IMSEn**
  - Remove **MMSEn**  because it's slower.
- **FuzzyEn**, and **FuzzyApEn**
  - Remove **FuzzyApEn**  because it's slower.
- **SVDEn**, and **FuzzycApEn**
  - Remove **FuzzycApEn**  because it's slower.
- **CPEn**, and **CRPEn**
  - Remove **CPEn**  to keep the Renyi entropy.
- **NLDFD**, and **RR**
  - Remove **RR**  because it's slower.




```{r eval=FALSE, message=FALSE, warning=FALSE, cache=FALSE, include=FALSE}
# Duplicates
# ===========
averagetime <- arrange(summarize(group_by(df, Index), Duration = mean(Duration)), Duration)

filter(averagetime, Index %in% c("CREn (D)", "PFD (D)", "ShanEn (D)"))
filter(averagetime, Index %in% c("ShanEn (B)", "CREn (B)"))
filter(averagetime, Index %in% c("ShanEn (r)", "PFD (r)", "CREn (r)"))
filter(averagetime, Index %in% c("ShanEn (C)", "PFD (C)", "CREn (C)"))
filter(averagetime, Index %in% c("CREn (10)", "CREn (100)"))
filter(averagetime, Index %in% c("SVDEn", "FI"))
filter(averagetime, Index %in% c("PSDFD (Hasselman2013)", "PSDFD (Voss1998)"))
filter(averagetime, Index %in% c("MMSEn", "IMSEn"))
filter(averagetime, Index %in% c("H (corrected)", "H (uncorrected)"))
filter(averagetime, Index %in% c("FuzzyEn", "FuzzyApEn"))
filter(averagetime, Index %in% c("RCMSEn", "FuzzyRCMSEn"))
filter(averagetime, Index %in% c("SVDEn", "FuzzycApEn"))
filter(averagetime, Index %in% c("CPEn", "CRPEn"))
filter(averagetime, Index %in% c("NLDFD", "RR"))
```

### Correlation

```{r message=FALSE, warning=FALSE, cache=FALSE, fig.width=16, fig.height=15}
data <- data |>
  select(
    -`H (corrected)`,
    -`CREn (100)`,
    -`PowEn`
  )


# data <- data |>
#   select(
#     -`CREn (B)`,
#     -`CREn (D)`, -`ShanEn (D)`,
#     -`CREn (r)`, -`ShanEn (r)`,
#     -`CREn (C)`, -`ShanEn (C)`,
#     -`CREn (100)`,
#     -`PowEn`,
#     -`PSDFD (Voss1998)`,
#     -`RangeEn (A)`, -`RangeEn (Ac)`,
#     -FI,
#     -MMSEn,
#     -`H (corrected)`
#     -FuzzyApEn,
#     -FuzzycApEn,
#     -CPEn,
#     -RR,
#     -MFDFA_HDelta,
#     -FuzzyRCMSEn
#     -`CREn (1000)`,
#     -`CREn (100)`,
#     -RQA_VEn, -RQA_LEn
#   )

cor <- get_cor(data, plot=TRUE)
```

We ran a Pearson correlation analysis using the *correlation* R package [@correlationArticle, @seeArticle, @parametersArticle, @modelbasedPackage].

Complexity indices, despite their multitude, their unicities and specificities, do indeed share similarities. They form clusters (some indices, by design, index the predictability, whereas other the randomness).

### Factor Analysis

```{r message=FALSE, warning=FALSE, fig.width=14, fig.height=7}
r <- correlation::cor_smooth(as.matrix(cor))

plot(parameters::n_factors(data, cor = r, n_max=20))
# plot(parameters::n_components(data, cor = r))
```


```{r message=FALSE, warning=FALSE, fig.width=12, fig.height=18}
rez <- parameters::factor_analysis(data, cor = r, n = 12, rotation = "varimax", sort = TRUE, fm="mle")
# rez <- parameters::principal_components(data, n = 15, sort = TRUE)
# rez

col <- gsub('[[:digit:]]+', '', names(rez)[2])
closest <- colnames(select(rez, starts_with(col)))[apply(select(rez, starts_with(col)), 1, \(x) which.max(abs(x)))]


loadings <- attributes(rez)$loadings_long |>
  mutate(
    Loading = Loading,
    Component = fct_relevel(Component, rev(names(select(rez, starts_with(col))))),
    Variable = fct_rev(fct_relevel(Variable, rez$Variable))
  )

colors <- setNames(see::palette_material("rainbow")(length(levels(loadings$Component))), levels(loadings$Component))

# Sort by sign too
names(closest) <- rev(levels(loadings$Variable))

idx_order <- loadings |>
  mutate(Closest = closest[as.character(loadings$Variable)],
         Sign = sign(Loading)) |>
  filter(Component == Closest) |>
  arrange(desc(Component), desc(Sign), desc(abs(Loading))) |>
  pull(Variable) |>
  as.character()

separations <- table(closest)[intersect(levels(loadings$Component), unique(closest))]


p1 <- loadings |>
  mutate(Variable = fct_relevel(Variable, rev(idx_order))) |>
  # filter(Variable == "CD") |>
  ggplot(aes(x = Variable, y = Loading)) +
  geom_bar(aes(fill = Component), stat = "identity") +
  geom_vline(xintercept = c("SD", "Length", "Noise", "Random"), color = "red") +
  geom_vline(xintercept = head(cumsum(separations), -1) + 0.5) +
  scale_y_continuous(expand = c(0, 0)) +
  scale_fill_material_d("rainbow") +
  coord_flip() +
  theme_minimal() +
  guides(fill = guide_legend(reverse = TRUE)) +
  labs(x = NULL) +
  theme(
    axis.text.y = element_text(
      color = rev(colors[closest]),
      face = rev(ifelse(idx_order %in% c("SD", "Length", "Noise", "Random"), "italic", "plain")),
      hjust = 0.5
    ),
    axis.text.x = element_blank(),
    plot.title = element_text(hjust = 0.5),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  )

p2 <- order |>
  mutate(Duration = 1 + Duration * 10000) |>
  filter(Index %in% loadings$Variable) |>
  mutate(Index = fct_relevel(Index, levels(loadings$Variable)),
         Duration = ifelse(is.na(Duration), 0, Duration)) |>
  ggplot(aes(x = log10(Duration), y = Index)) +
  geom_bar(aes(fill = log10(Duration)), stat = "identity") +
  geom_hline(yintercept = head(cumsum(separations), -1) + 0.5) +
  scale_x_reverse(expand = c(0, 0)) +
  # scale_x_log10(breaks = 10**seq(0, 4), labels = function(x) sprintf("%g", x), expand=c(0, 0)) +
  scale_y_discrete(position = "right") +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Computation Time", y = NULL) +
  theme_minimal() +
  theme(
    axis.text.y = element_blank(),
    axis.text.x = element_blank(),
    plot.title = element_text(hjust = 0.5),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  )

(p2 | p1) + patchwork::plot_annotation(title = "Computation Time and Factor Loading", theme = theme(plot.title = element_text(hjust = 0.5, face = "bold")))
```

#### Graph

```{r graph, message=FALSE, warning=FALSE, fig.width=14, fig.height=7, include=FALSE, eval=FALSE}
library(ggraph)

g <- cor |>
  cor_lower() |>
  mutate(width = abs(r),
         edgecolor = as.character(sign(r))) |>
  filter(!Index %in% c("SD", "Length", "Random", "Frequency")) |>
  tidygraph::as_tbl_graph(directed=FALSE)

p1 <- g |>
  mutate(importance = tidygraph::centrality_authority(weights = abs(r)),
         group = as.factor(tidygraph::group_louvain(weights = abs(r)))) |>
  tidygraph::activate("edges") |>
  filter(abs(r) > 0.66) |>
  tidygraph::activate("nodes") |>
  filter(!tidygraph::node_is_isolated()) |>
  mutate(colors = closest[name]) |> 
  ggraph(layout = 'circle') +  # fr # lgl # drl # kk
  ggraph::geom_edge_arc(aes(edge_width=width, edge_colour=edgecolor), strength=0.3, alpha=0.66) +
  ggraph::geom_node_point(aes(size = importance, color = colors)) +
  # ggraph::geom_node_label(aes(label = name, fill = group, size=importance), repel=TRUE) +
  ggraph::geom_node_text(aes(x = x*1.05, y=y*1.05, label = name, angle = -((-node_angle(x, y)+90)%%180)+90, hjust='outward')) +
  scale_edge_color_manual(values = c("1" = "#2E7D32", "-1"="#C62828"), guide = "none") +
  scale_edge_width_continuous(range = c(0.005, 0.66), guide = "none") +
  scale_size_continuous(range = c(0.1, 2), guide = "none") +
  scale_fill_material_d(guide= "none") +
  scale_colour_material_d(guide= "none") +
  ggraph::theme_graph() +
  expand_limits(x = c(-1.5, 1.5), y = c(-1.5, 1.5))
p1
```


#### Hierarchical Clustering


```{r message=FALSE, warning=FALSE, fig.width=14, fig.height=14}
dat <- data |> 
  select(-SD, -Length, -Random, -Frequency) |> 
  t() |> 
  as.data.frame()

p2 <- hclust(dist(dat), method = "ward.D2") |>
  create_layout(layout = 'dendrogram', circular = TRUE, repel=TRUE) |> 
  attr("graph") |> 
  tidygraph::activate("nodes") |>
  ggraph(layout = 'dendrogram', circular = TRUE, repel=TRUE) + 
  geom_edge_elbow() +
  ggraph::geom_node_text(aes(x = x*1.05, y=y*1.05, label = label, angle = -((-node_angle(x, y)+90)%%180)+90, hjust='outward')) +
  coord_fixed() +
  ggraph::theme_graph() +
  expand_limits(x = c(-1.5, 1.5), y = c(-1.5, 1.5))

p1 + p2
```


<!-- ### Misc -->


<!-- #### Sensitivity to Signal Length -->


<!-- ```{r message=FALSE, warning=FALSE, include=FALSE, cache=FALSE} -->
<!-- model <- lm(Result ~ Index / poly(Length, 2), data = filter(df, !Index %in% c("SD", "Length", "Noise", "Random"))) -->

<!-- parameters::parameters(model, keep = "poly.*1") |> -->
<!--   arrange(desc(abs(Coefficient))) |> -->
<!--   filter(p < .05) -->

<!-- estimate_relation(model) |> -->
<!--   ggplot(aes(x = Length, y = Predicted)) + -->
<!--   geom_ribbon(aes(ymin = CI_low, ymax = CI_high, fill = Index), alpha = 0.1) + -->
<!--   geom_line(aes(color = Index)) + -->
<!--   geom_point2( -->
<!--     data = filter(df, Index != "SD"), -->
<!--     aes(y = Result, color = Index), -->
<!--     alpha = 0.1, size = 2 -->
<!--   ) + -->
<!--   scale_fill_manual(values = colors) + -->
<!--   scale_color_manual(values = colors) + -->
<!--   theme(legend.position = "none") + -->
<!--   facet_wrap(~Index, scales = "free") -->
<!-- ``` -->

<!-- #### Sensitivity to Noise -->

<!-- ```{r message=FALSE, warning=FALSE, include=FALSE, cache=FALSE} -->
<!-- model <- lm(Result ~ Index / poly(Noise_Intensity, 2), data = filter(df, !Index %in% c("SD", "Length", "Noise", "Random"))) -->

<!-- parameters::parameters(model, keep = "poly.*1") |> -->
<!--   arrange(abs(p)) |> -->
<!--   filter(p < .05) -->

<!-- estimate_relation(model) |> -->
<!--   ggplot(aes(x = Noise_Intensity, y = Predicted)) + -->
<!--   geom_ribbon(aes(ymin = CI_low, ymax = CI_high, fill = Index), alpha = 0.1) + -->
<!--   geom_line(aes(color = Index)) + -->
<!--   geom_point2( -->
<!--     data = filter(df, Index != "SD"), -->
<!--     aes(y = Result, color = Index), -->
<!--     alpha = 0.1, size = 2 -->
<!--   ) + -->
<!--   scale_fill_manual(values = colors) + -->
<!--   scale_color_manual(values = colors) + -->
<!--   theme(legend.position = "none") + -->
<!--   facet_wrap(~Index, scales = "free_y") -->
<!-- ``` -->









<!-- ### Visualization -->

<!-- ```{r message=FALSE, warning=FALSE, fig.width=14, fig.height=7} -->
<!-- model <- # lm(Result ~ Index * (Length + Noise_Type + poly(Noise_Intensity, 2) * Signal), -->
<!--          mgcv::gam(Result ~ s(Noise_Intensity, by = interaction(Index, Signal)) + Index * (Length + Noise_Type), -->
<!--             data=df |> -->
<!--               filter(Index %in% c( -->
<!--                 "NLDFD", -->
<!--                 "SVDEn", -->
<!--                 # "SampEn", -->
<!--                 "PEn", -->
<!--                 # "FuzzyEn", -->
<!--                 # "WPEn", -->
<!--                 "K2En", -->
<!--                 # "PSDFD (Hasselman 2013)", -->
<!--                 "BubbEn", -->
<!--                 "MSPEn", -->
<!--                 "MFDFA_Increment" -->
<!--                 # "MFDFA_Width", -->
<!--                 # "MFDFA_Delta" -->
<!--                 )) |> -->
<!--               mutate(Noise_Type = as.factor(Noise_Type))) -->

<!-- # estimate_means(model, at = c("Index", "Signal", "Noise_Intensity", "Noise_Type")) |> -->
<!-- #   ggplot(aes(y = Mean, x = Noise_Intensity)) + -->
<!-- #   geom_line(aes(color = Index, size=Noise_Type)) + -->
<!-- #   facet_grid(Noise_Type~Signal) + -->
<!-- #   scale_linetype_manual(values = c("-2" = 3, "-1" = 4, "0" = 2, "1" = 5, "2" = 1)) + -->
<!-- #   scale_size_manual(values = c("-2" = 0.2, "-1" = 0.4, "0" = 0.6, "1" = 0.8, "2" = 1)) + -->
<!-- #   theme_classic() + -->
<!-- #   theme(panel.grid.major = element_line(colour = "#EEEEEE")) -->


<!-- estimate_means(model, at = c("Index", "Signal", "Noise_Intensity")) |> -->
<!--   ggplot(aes(y = Mean, x = Noise_Intensity)) + -->
<!--   geom_line(aes(color = Index)) + -->
<!--   facet_grid(~Signal) + -->
<!--   scale_linetype_manual(values = c("-2" = 3, "-1" = 4, "0" = 2, "1" = 5, "2" = 1)) + -->
<!--   theme_classic() + -->
<!--   theme(panel.grid.major = element_line(colour = "#EEEEEE")) -->
<!-- ``` -->

## Discussion

As complexity science grows in size and application, a systematic approach to compare their "performance" becomes necessary to increase the clarity and structure of the field. The word *performance* is here to be understood in a relative sense, as any such endeavor faces the "hard problem" of complexity science. The indices are sensitive to specific objective properties of a signal that we consider part of over-arching concepts such as "complex" and "chaotic". But it is unclear how these high-level concepts transfer back, in a top-down fashion, into a combination of lower-level features, such as short-term vs. long-term variability, auto-correlation, information, randomness, and so on. As such, it is conceptually complicated to benchmark complexity measures against "objectively" complex vs. non-complex signals. In other words, we know that different characteristics can contribute to the "complexity" of a signal, but there is not a one-to-one correspondence between the latter and the former. 

Hence the choice of the current paradigm, in which we generated different types of signals to which we systematically added different types and amount of perturbations. However, we did not seek at measuring how complexity indices can discriminate between these features or systems, nor did we attempt at mimicking real-life signals or scenarios. The goal was instead to generate enough variability to reliably map the relationships between the indices. 

The plurality of underlying components of empirical complexity (what is measured by complexity indices) seems to be somewhat confirmed by our results, showing that complexity indices are more or less sensitive to various orthogonal latent dimensions. One of the limitation of the current study has to do with the limited possibilities of interpretation of these underlying dimensions, and future studies are needed to discuss them. 

Indices that were highlighted as reliably providing information about underlying dimensions ata low computational cost include... 


## References
